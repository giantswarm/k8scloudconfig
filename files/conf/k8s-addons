#!/bin/bash

export HOSTNAME=$(hostname | tr '[:upper:]' '[:lower:]')
export KUBECONFIG=/etc/kubernetes/kubeconfig/addons.yaml

# wait for healthy master
while [ "$(kubectl get nodes $HOSTNAME -o jsonpath='{.metadata.name}')" != "$HOSTNAME" ]; do sleep 1 && echo 'Waiting for healthy k8s'; done

# label namespaces (required for network egress policies)
NAMESPACES="default kube-system" 
for namespace in ${NAMESPACES}
do
    if ! kubectl get namespaces -l name=${namespace} | grep ${namespace}; then
        while
            kubectl label namespace ${namespace} name=${namespace} --overwrite=true
            [ "$?" -ne "0" ]
        do
            echo "failed to label namespace ${namespace}, retrying in 5 sec"
            sleep 5s
        done
    fi
done

# apply Security bootstrap (RBAC and PSP)
SECURITY_FILES=""
SECURITY_FILES="${SECURITY_FILES} rbac_bindings.yaml"
SECURITY_FILES="${SECURITY_FILES} rbac_roles.yaml"
SECURITY_FILES="${SECURITY_FILES} psp_policies.yaml"
SECURITY_FILES="${SECURITY_FILES} psp_roles.yaml"
SECURITY_FILES="${SECURITY_FILES} psp_binding.yaml"

for manifest in $SECURITY_FILES
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done

{{- if not .Etcd.HighAvailability }}
# check for other master and remove it
THIS_MACHINE=$HOSTNAME
for master in $(kubectl get nodes --no-headers=true --selector role=master | awk '{print $1}')
do
    if [ "$master" != "$THIS_MACHINE" ]; then
        kubectl delete node $master
    fi
done
{{- end }}

# wait for etcd dns (return code 35 is bad certificate which is good enough here)
while
    curl "https://{{ .Cluster.Etcd.Domain }}:{{ .Etcd.ClientPort }}" -k 2>/dev/null >/dev/null
    RET_CODE=$?
    [ "$RET_CODE" -ne "35" ]
do
    echo "Waiting for etcd to be ready . . "
    sleep 3s
done

# create kube-proxy configmap
while
    kubectl create configmap kube-proxy --from-file=kube-proxy.yaml=/srv/kube-proxy-config.yaml -o yaml --dry-run | kubectl apply -n kube-system -f -
    [ "$?" -ne "0" ]
do
    echo "failed to configure kube-proxy from /srv/kube-proxy-config.yaml, retrying in 5 sec"
    sleep 5s
done

# The k8s-app selector was changed to app.kubernetes.io/name in k8scloudconfig v6.0.3/v6.1.0.
# This change was reverted in k8scloudconfig v6.1.1+.
# matchLabels is immutable in apps/v1 so `kubectl apply -f kube-proxy-ds.yaml` will fail
# when upgrading clusters using k8scloudconfig v6.0.3/v6.1.0.
# To mitigate this without causing downtime, we can orphan the existing pods with `kubectl delete --cascade=false`
# The pods will continue to run and the new daemonset will inherit the pods so they can be
# upgraded if needed in a graceful manner.
if [[ $(kubectl get ds -n kube-system kube-proxy -o jsonpath='{.spec.selector.matchLabels}') == "map[app.kubernetes.io/name:kube-proxy]" ]]; then
  echo "Orphaning kube-proxy pods to be inherited by new deployment"
  kubectl delete ds -n kube-system kube-proxy --cascade=false
fi

# install kube-proxy
PROXY_MANIFESTS="kube-proxy-sa.yaml kube-proxy-ds.yaml"
for manifest in $PROXY_MANIFESTS
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done
echo "kube-proxy successfully installed"

# restart ds to apply config from configmap
kubectl delete pods -l app.kubernetes.io/name=kube-proxy -n kube-system

# See kube-proxy comment above for an explanation
if [[ $(kubectl get ds -n kube-system calico-node -o jsonpath='{.spec.selector.matchLabels}') == "map[app.kubernetes.io/name:calico-node]" ]]; then
  echo "Orphaning calico-node pods to be inherited by new deployment"
  kubectl delete ds -n kube-system calico-node --cascade=false
fi

{{ if .CalicoPolicyOnly -}}
## Apply Calico with network policy features only
CNI_FILE="calico-policy-only.yaml"
{{ if .EnableAWSCNI -}}
## Apply AWS VPC CNI
CNI_FILE="${CNI_FILE} aws-cni.yaml"
{{ end -}}
{{ else -}}
## Apply Calico with all its components
CNI_FILE="calico-all.yaml"
# See kube-proxy comment above for an explanation
if [[ $(kubectl get deploy -n kube-system calico-kube-controllers -o jsonpath='{.spec.selector.matchLabels}') == "map[app.kubernetes.io/name:calico-kube-controllers]" ]]; then
  echo "Orphaning calico-kube-controllers pods to be inherited by new deployment"
  kubectl delete deploy -n kube-system calico-kube-controllers --cascade=false
fi
{{ end -}}

for manifest in ${CNI_FILE}
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done

while
    echo "Waiting for calico-node to be ready..."
    kubectl -n kube-system -l app.kubernetes.io/name=calico-node wait --for=condition=Ready --timeout=1m pods
    [ "$?" -ne "0" ]
do
    echo "calico-node was not ready after 1 minute"
done

echo "calico-node is ready"

# apply default storage class
if [ -f /srv/default-storage-class.yaml ]; then
    while
        kubectl apply -f /srv/default-storage-class.yaml
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/default-storage-class.yaml, retrying in 5 sec"
        sleep 5s
    done
else
    echo "no default storage class to apply"
fi

# apply priority classes:
PRIORITY_CLASSES_FILE="priority_classes.yaml"

while
    kubectl apply -f /srv/$PRIORITY_CLASSES_FILE
    [ "$?" -ne "0" ]
do
    echo "failed to apply /srv/$PRIORITY_CLASSES_FILE, retrying in 5 sec"
    sleep 5s
done

# apply network policies:
NETWORK_POLICIES_FILE="network_policies.yaml"
NAMESPACES="kube-system giantswarm"
for namespace in ${NAMESPACES}; do
    while
      kubectl apply -f /srv/$NETWORK_POLICIES_FILE -n $namespace
      [ "$?" -ne "0" ]
    do
      echo "failed to apply /srv/$NETWORK_POLICIES_FILE, retrying in 5 sec"
      sleep 5s
    done
done

# apply k8s addons
MANIFESTS=""
{{ range .ExtraManifests -}}
MANIFESTS="${MANIFESTS} {{ . }}"
{{ end -}}
{{ if not .DisableIngressControllerService -}}
MANIFESTS="${MANIFESTS} ingress-controller-svc.yaml"
{{ end -}}

for manifest in $MANIFESTS
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done
echo "Addons successfully installed"
