#!/bin/bash

export HOSTNAME=$(hostname | tr '[:upper:]' '[:lower:]')

export KUBECONFIG=/etc/kubernetes/kubeconfig/addons.yaml

# wait for healthy master
while [ "$(kubectl get nodes $HOSTNAME -o jsonpath='{.metadata.name}')" != "$HOSTNAME" ]; do export HOSTNAME=$(hostname | tr '[:upper:]' '[:lower:]') && sleep 1 && echo 'Waiting for healthy k8s'; done

# label namespaces (required for network egress policies)
NAMESPACES="default kube-system" 
for namespace in ${NAMESPACES}
do
    if ! kubectl get namespaces -l name=${namespace} | grep ${namespace}; then
        while
            kubectl label namespace ${namespace} name=${namespace} --overwrite=true
            [ "$?" -ne "0" ]
        do
            echo "failed to label namespace ${namespace}, retrying in 5 sec"
            sleep 5s
        done
    fi
done

# apply Security bootstrap (RBAC and PSP)
SECURITY_FILES=""
SECURITY_FILES="${SECURITY_FILES} rbac_bindings.yaml"
SECURITY_FILES="${SECURITY_FILES} rbac_roles.yaml"
SECURITY_FILES="${SECURITY_FILES} psp_roles.yaml"
SECURITY_FILES="${SECURITY_FILES} psp_binding.yaml"

for manifest in $SECURITY_FILES
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done

{{- if not .Etcd.HighAvailability }}
# check for other master and remove it
THIS_MACHINE=$HOSTNAME
for master in $(kubectl get nodes --no-headers=true --selector role=master | awk '{print $1}')
do
    if [ "$master" != "$THIS_MACHINE" ]; then
        kubectl delete node $master
    fi
done
{{- end }}

# wait for etcd dns (return code 35 is bad certificate which is good enough here)
while
    curl "https://{{ .Cluster.Etcd.Domain }}:{{ .Etcd.ClientPort }}" -k 2>/dev/null >/dev/null
    RET_CODE=$?
    [ "$RET_CODE" -ne "35" ]
do
    echo "Waiting for etcd to be ready . . "
    sleep 3s
done

{{ if not .DisableKubeProxy }}
# create kube-proxy configmap
while
    kubectl create configmap kube-proxy --from-file=kube-proxy.yaml=/srv/kube-proxy-config.yaml -o yaml --dry-run=client | kubectl apply -n kube-system -f -
    [ "$?" -ne "0" ]
do
    echo "failed to configure kube-proxy from /srv/kube-proxy-config.yaml, retrying in 5 sec"
    sleep 5s
done

# install kube-proxy
PROXY_MANIFESTS="kube-proxy-sa.yaml kube-proxy-ds.yaml"
for manifest in $PROXY_MANIFESTS
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done
echo "kube-proxy successfully installed"

# restart ds to apply config from configmap
kubectl delete pods -l app.kubernetes.io/name=kube-proxy -n kube-system
{{ end }}

{{ if not .DisableCalico }}
# install calico CRDS
PROXY_MANIFESTS="calico-crd-installer-rbac.yaml calico-crd-installer.yaml"
for manifest in $PROXY_MANIFESTS
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done
echo "calico CRD installer successfully applied"

while
    kubectl wait --for=condition=complete --timeout=1m -n kube-system job -lapp.kubernetes.io/name=calico-crd-installer
    [ "$?" -ne "0" ]
do
    echo "Failed waiting for crd installer job to be completed, retrying in 5 sec"
    sleep 5s
done

# Ensure calico CRDs are created
kubectl get crd ippools.crd.projectcalico.org || (echo "Calico CRD was not found. This is unexpected, exiting." ; exit 1)

{{ if .CalicoPolicyOnly }}
## Apply Calico with network policy features only
CNI_FILE="calico-policy-only.yaml"
{{ if .EnableAWSCNI }}
## Apply AWS VPC CNI
CNI_FILE="${CNI_FILE} aws-cni.yaml"
{{ end }}
{{ else }}
## Apply Calico with all its components
CNI_FILE="calico-all.yaml"
{{ end }}

for manifest in ${CNI_FILE}
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done

while
    echo "Waiting for calico-node to be ready..."
    kubectl -n kube-system -l app.kubernetes.io/name=calico-node wait --for=condition=Ready --timeout=1m pods
    [ "$?" -ne "0" ]
do
    echo "calico-node was not ready after 1 minute"
done

echo "calico-node is ready"

{{ if not .CalicoPolicyOnly }}
# TODO: remove migrator once all clusters have been migrated to kubernetes datastore
kubectl apply -f /srv/calico-datastore-migrator-post.yaml
kubectl wait --for=condition=complete --timeout=1m -n kube-system job -lapp.kubernetes.io/name=calico-datastore-migrator-post
{{ end }}

kubectl delete -f /srv/calico-crd-installer-rbac.yaml
{{ end }}

# apply default storage class
if [ -f /srv/default-storage-class.yaml ]; then
    while
        kubectl apply -f /srv/default-storage-class.yaml
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/default-storage-class.yaml, retrying in 5 sec"
        sleep 5s
    done
else
    echo "no default storage class to apply"
fi

# apply priority classes:
PRIORITY_CLASSES_FILE="priority_classes.yaml"

while
    kubectl apply -f /srv/$PRIORITY_CLASSES_FILE
    [ "$?" -ne "0" ]
do
    echo "failed to apply /srv/$PRIORITY_CLASSES_FILE, retrying in 5 sec"
    sleep 5s
done

# apply network policies:
NETWORK_POLICIES_FILE="network_policies.yaml"
NAMESPACES="kube-system giantswarm"
for namespace in ${NAMESPACES}; do
    while
      kubectl apply -f /srv/$NETWORK_POLICIES_FILE -n $namespace
      [ "$?" -ne "0" ]
    do
      echo "failed to apply /srv/$NETWORK_POLICIES_FILE, retrying in 5 sec"
      sleep 5s
    done
done

# apply k8s addons
MANIFESTS=""
{{ range .ExtraManifests -}}
MANIFESTS="${MANIFESTS} {{ . }}"
{{ end -}}
{{ if not .DisableIngressControllerService -}}
MANIFESTS="${MANIFESTS} ingress-controller-svc.yaml"
{{ end -}}

for manifest in $MANIFESTS
do
    while
        kubectl apply -f /srv/$manifest
        [ "$?" -ne "0" ]
    do
        echo "failed to apply /srv/$manifest, retrying in 5 sec"
        sleep 5s
    done
done
echo "Addons successfully installed"
